Dimensionality Reduction 

o Feature Selection 

§ How, why, and what is the Importance?

- Reduces Overfitting
  - Makes model more general 
  - Gets rid of ‘noise’

- Gets rid of redundancies in the data
- Makes results easier to interpret
- Makes collecting new data more efficient



o Curse of Dimensionality 

- More dimensions means sparser data
  - Exponential increase in volume
  - Large datasets are needed to learn properly

- “Noise” from irrelevant dimensions may swamp the “signal” from relevant dimensions 
- Distance metrics may become increasingly less useful with more dimensions 
- Probability distributions start behaving non-intuitively in high-dimensional spaces



o PCA 

§ What is the goal and generally how does it work?

Goal: Reduce a set of numerical variables

§ What are the properties of the algorithm?

§ Be able to draw the principal components on a scatter plot of points.