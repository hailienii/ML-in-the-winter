# Anomaly Detection

1. Outlier / Anomaly:
The set of data points that are considerably different than the remainder of the data (anomalies are generated by a “different mechanism”)
2. Real-world uses
   - Fraud detection
   - Healthcare informatics

3. 1) General framework (scheme) for outlier/anomaly detection
  General Steps

  - Build a profile of the “normal” behavior
    I.e., patterns or summary statistics for the overall population

  - Use the “normal” profile to detect anomalies
    I.e., observations whose characteristics differ significantly from the normal profile

  Detection schemes

  - Graphical  
  - Statistical/model based 
  - Distance/proximity based

4. Challenges of Outlier Detection

- Modeling regular objects and outliers properly

  - Hard to enumerate all possible regular behaviors in an application 
  - The border between regular and outlier objects is often a gray area 
  - More complex outlier behavior: collective outliers, contextual outliers

-  Application-specificity in outlier detection

  - Choice of distance measure among objects and the model of relationship among objects are often application-dependent

- Handling noise in outlier detection

  - Noise may distort normal objects, blurring the difference between regular objects and outliers

- Understandability

  - Why these are outliers? (I.e., justification of the detection)

    • E.g., poor data quality, measurement malfunctions, manual entry errors, correct but abnormal data

  - Specify the degree of an outlier

    • The unlikelihood of the object being generated by a normal mechanism

- Supervised outlier detection methods
  - Can be used, if descriptions (labels) of anomalous data are available 
  - Otherwise, it is typically an unsupervised (exploratory) learning task



3) What are the assumptions?
There are considerably more “normal” observations than “abnormal” observations (outliers/anomalies) in the data


4. Graphical vs Model Based vs Distance Based 

  |                                                              | Assuption                                                    | Challenges                                                   | Benefits                                                     |
  | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | Graphical                                                    |                                                              |                                                              |                                                              |
  | BoxPlot (1D)  <br/>Scatter Plot (2D) <br/>Spinning 3D scatterplot (3D) |                                                              | Time consuming<br/>subjective<br/>Difficult to use with higher dimensionality | Intuitive, use the power of human cognition                  |
  |                                                              |                                                              |                                                              |                                                              |
  | **Model Based**(Statistical Approaches)                      |                                                              |                                                              |                                                              |
  | Quartile-Based Technique                                     | The regular data follow some statistical model (a stochastic model)<br/>- The data not following the model are outliers<br/>- Lots of different models are available | - Most of the tests are for a single attribute <br/>- Data distribution may not be known<br/>- For high dimensional data, it may be difficult to estimate the true distribution |                                                              |
  |                                                              |                                                              |                                                              |                                                              |
  | Non-Parametric Methods  Histogram-based approach             |                                                              | Hard to choose an appropriate bin size for histogram<br/>- Too small bin size: regular objects in empty/rare bins (false positive)<br/>- Too big bin size: outliers in some frequent bins (false negative) | Makes fewer assumptions about the data – can be applicable in more scenarios |
  | **Distance Based**                                           |                                                              |                                                              |                                                              |
  | Nearest-neighbor-based                                       |                                                              |                                                              |                                                              |
  | Density-based                                                |                                                              |                                                              |                                                              |
  | Clustering-based                                             |                                                              |                                                              |                                                              |

  

1. Simple Statistical Approach: 

Quartile-Based Technique

- Let Q1 and Q3 are first and third quartiles
- Let *IQR* = |Q1 – Q3|

• Mild outliers (1 out of 150 in normally distributed data):

 < Q1 – 1.5  *IQR*  > Q3 + 1.5  *IQR*

• Extreme outliers (1 out of 425,000 in n. d. data):

< Q1 – 3  *IQR*  > Q3 + 3  *IQR*

• A number of more sophisticated statistical (model-based) approaches



<img src='https://miro.medium.com/max/24000/1*IZ2II2HYKeoMrdLU5jW6Dw.png'>



2. Advanced Statistical / model-based

**Generalized ESD**(Generalized Extreme Studentized Deviate):
It is used on univariate data which follows an approximately normal distribution, and can be used to detect one or more outliers.

remember always re-calculate mean and sd after throwing out the questionable outlier!

![IMG_D856292CEEBB-1.jpeg](https://i.loli.net/2019/11/25/kRni8hIGaJAEmW1.jpg)



3. Spatial and temporal anomaly detection

   Time series analysis: the expected value for time step t is a function of the values for time steps 1 through t – 1.
   
   Spatial regression: the expected value for location s is a function of the values for all other locations.









1. Nearest-Neighbor-Based Approach



2. Density-Based Approach
   - Finds **local outliers**, i.e., by comparing data points to their local neighborhoods, instead of looking at the global data distribution
   - Intuition: The density around an outlier object is significantly different from the density around its neighbors
   - Method: Use the relative density of an object against its neighbors as the indicator of the degree of the object being outliers



**Local Outlier Factor (LOF)**

**算法原理如下**：

1. 计算k-distance of p：计算点`p`的第`k`距离，也就距离样本点`p`第`k`远的点的距离，不包括`p`;

2. 计算k-distance neighborhood of p：计算点`p`的第`k`邻域距离，就是`p`的第`k`距离以内的所有点，包括第`k`距离;

3. 计算reach-distance：可达距离，若小于第`k`距离，则可达距离为第`k`距离，若大于第k距离，则可达距离为真实距离，公式如下(说明:$d(p,o)$为`p`到`o`的距离)：
   $$
   reach-distance_k(p,o)=max\{k−distance(o),d(p,o)\} \tag{1}\label{1}
   $$
   点`o`到点`p`的第`k`可达距离，至少是点`o`的第`k`距离，或者为`o`与`p`间的真实距离。

4. 计算local reachability density：局部可达密度
   $$
   lrd_k(p)=\frac{1}{ \frac{1}{|Nk(p)|} \sum _{o \in Nk(p)}reach−distance_k(p,o)}
   \tag{2}\label{2}
   $$
   表示点`p`的第`k`邻域内点到点`p`的平均可达距离的倒数。

5. 计算local outlier factor:局部离群因子.
   $$
   LOF_k(p)=\frac{1}{ |Nk(p)|} ∑_{o\in Nk(p)} \frac{lrd_k(o) }{lrd_k(p)} = \frac{∑ _{o\in Nk(p)} lrd_k(o)}{|Nk(p)| } \cdot \frac{1}{lrd_k(p)}
   \tag{3}\label{3}
   $$
   表示点`p`的邻域点`Nk(p)`的局部可达密度与点`p`的局部可达密度之比的平均数。





3. Clustering-Based Approaches

Assumption: Regular data belongs to large and dense clusters, whereas outliers belong to small or sparse clusters, or do not belong to any clusters

Main Idea:

• First, cluster the data using some standard clustering technique

​	o E.g., k-means, DBSCAN, etc.

• A given data point is an outlier if 

(a) it does not belong to any cluster (e.g., in case of DBSCAN), 

(b) belongs to a very small cluster, 

or (c) it is far from its closest cluster center (as compared to an average data point)

• One common approach in anomaly detection (e.g., intrusion detection systems):

​	o Get “normal” data and cluster it 

​	o Compare newly incoming data to normal clusters



• Strengths

o Many clustering techniques available 

o Work for many types of data 

o Clusters can be regarded as summaries of the data 

o Once the cluster are obtained, need only compare any object against the clusters to determine whether it is an outlier (fast)

• Weaknesses

o Effectiveness depends highly on the clustering method used – it may not be optimized for outlier detection 

o High computational cost

• I.e., need to first find clusters

• There are some techniques that try to mitigate this cost







3. Advanced Model Based 

§ Understand the concept of using the comparison of expectation to observation for anomaly detection

§ Understand the general principles of spatial and temporal anomaly detection 



Understand the **key principles and main goals of Anomalous Pattern Detection** 

Main goal: to identify and characterize relevant subsets of a massive dataset, i.e. groups of records that differ from the rest of the data in an interesting way.

Principles: 

Cluster detection: find spatial areas or periods of time with more records than expected.

Event detection: is the recent data differently distributed than the past?

Key concept: A group of records may be highly anomalous or interesting even if none of the individual records is itself anomalous.



§ How is it different from anomaly detection?



§ How does **subset scanning** enable anomalous pattern detection?

•We can scan over subsets of the dataset in order to find those groups of records that correspond to a pattern.

​	Step 1: Compute score F(S, P) for each subset S = {x i } and for each pattern type P, where higher score means more likely to be a pattern.

​	Step 2: Consider the highest scoring potential patterns (S, P) and decide whether each actually represents a pattern.

​	There are many options for computing the score of a subset S.

- In the WSARE method (“What’s Strange About Recent Events”), we consider the subsets of the data defined by a one- or two-component rule R, and find rules where the current data is significantly different than the past.
- In the model-based anomalous pattern detection approach, we model the effects of each pattern type P on the affected subset of the data S. (Bayesian Networks are a popular means to model the data.)

Which patterns to report?

Option 1: Report the k highest scoring subsets, ordered by score.

The disadvantage of this approach is that the user is not informed whether any of the discovered patterns are likely to be relevant. However, this may be acceptable in monitoring systems or scientific discovery applications where the user is willing to evaluate a fixed number of potential patterns.

Option 2(better for anomalies): Perform hypothesis tests, and report all significant patterns (S, P).

In the hypothesis testing framework, we must adjust for the fact that we’re performing so many tests. Otherwise we will report too many false positives!

In model-based approaches, one way to do this is randomization: we generate a large number of simulated datasets assuming the null model, and compare the scores of the potential patterns in the real dataset to the highest scoring patterns in the simulated data.

An alternative is to adjust the p-value threshold for each test based on the number of tests performed (e.g. Bonferroni threshold = .05 / # tests)



Option 3: Compute the posterior probability of each hypothesis H 1 (S, P).

In a Bayesian framework, we must spread the prior probability of a pattern over all possible hypotheses H 1 (S, P).

We then compute the likelihood of the data given each hypothesis H 1 (S, P), as well as the null hypothesis of no patterns, H 0 .

We can then compute the posterior probability of each hypothesis by Bayes’ Theorem:

Pr(H | D) = Pr(D | H) Pr(H) / Pr(D)



Which subsets to scan?

The most common approach is to use domain knowledge to restrict our search space: for example, in spatial cluster detection, we assume that a pattern will affect a spatially localized group of records, and often further restrict the cluster size and shape.

Another common approach is to perform a greedy search. For example, we grow subsets starting from each record, repeatedly adding the additional record that gives the highest scoring subset.





• Be able to explain WSARE and the Model-based pattern detection







o Interesting issue: anomaly detection vs. outlier detection