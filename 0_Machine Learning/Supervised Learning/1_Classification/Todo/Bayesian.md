# Generative Learning

A generative model first tries to learn how the data is generated by estimating P(x|y), which we can then use to estimate P(y|x) by using Bayes' rule.

## Gaussian Discriminant Analysis

**Setting** ― The Gaussian Discriminant Analysis assumes that yy and x|y=0x|y=0 and x|y=1are such that:

![image.png](https://i.loli.net/2019/10/18/rwt9OfRsCgc3qWE.png)

**Estimation** ― The following table sums up the estimates that we find when maximizing the likelihood:

| $\widehat{\phi}$                                        | $\widehat{\mu_j}\quad{\small(j=0,1)}$                   | $\widehat{\Sigma}$                                           |
| ------------------------------------------------------- | ------------------------------------------------------- | ------------------------------------------------------------ |
| $\displaystyle\frac{1}{m}\sum_{i=1}^m1_{\{y^{(i)}=1\}}$ | $\displaystyle\frac{1}{m}\sum_{i=1}^m1_{\{y^{(i)}=1\}}$ | $\displaystyle\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T$ |



## Naive Bayes

**Assumption** ― The Naive Bayes model supposes that the features of each data point are all independent:

$\boxed{P(x|y)=P(x_1,x_2,...|y)=P(x_1|y)P(x_2|y)...=\prod_{i=1}^nP(x_i|y)}$

**Solutions** ― Maximizing the log-likelihood gives the following solutions, with 

$k\in\{0,1\},l\in[\![1,L]\!]$

$\boxed{P(y=k)=\frac{1}{m}\times\#\{j|y^{(j)}=k\}}\quad\textrm{ and }\quad\boxed{P(x_i=l|y=k)=\frac{\#\{j|y^{(j)}=k\textrm{ and }x_i^{(j)}=l\}}{\#\{j|y^{(j)}=k\}}}$



*Remark: Naive Bayes is widely used for text classification and spam detection.*





